{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Introduction** **to** **Project**\n",
        "\n",
        "\n",
        "# üöÄ Cross-Platform Tech Trends Scraper\n",
        "\n",
        "## üìå Project Overview\n",
        "\n",
        "This project demonstrates a complete **end-to-end data collection and processing pipeline** using multiple web data acquisition techniques ‚Äî all fully free and applicable in real-world scenarios. It brings together **Requests + BeautifulSoup, Selenium, RapidAPI, and pandas** to collect, store, clean, and process web and API-derived data.\n",
        "\n",
        "You will learn how to:\n",
        "- Extract data from static HTML pages using Requests + BeautifulSoup\n",
        "- Scrape dynamic websites using Selenium\n",
        "- Fetch structured JSON data from a free RapidAPI endpoint\n",
        "- Store all gathered data in structured CSV files\n",
        "- Clean and normalize real, messy web data\n",
        "- Do basic processing and simple insights\n",
        "\n",
        "This workflow mirrors typical data-engineering and data-science pipelines, making it perfect for learning and portfolio purposes.\n",
        "\n",
        "---\n",
        "\n",
        "## üìå Data Sources and Methods\n",
        "\n",
        "| Source | Method | Output |\n",
        "|--------|--------|--------|\n",
        "| Hacker News Top Stories | Static scraping with `requests` + `BeautifulSoup` | `hacker_news.csv` |\n",
        "| Twitter Search Page (e.g., ‚ÄúAI‚Äù topic) | Dynamic scraping using `selenium` | `twitter_ai_tweets.csv` |\n",
        "| RapidAPI Free News API (e.g., AI or tech news) | API call using `requests` | `rapidapi_tech_news.csv` |\n",
        "\n",
        "---\n",
        "\n",
        "## üìå Project Steps\n",
        "\n",
        "### üß± 1. Requests + BeautifulSoup\n",
        "Use Requests to fetch web pages and BeautifulSoup to parse static HTML and extract structured data.\n",
        "\n",
        "### üåÄ 2. Selenium\n",
        "Use a headless Selenium browser to load dynamic content (like Twitter search results) that cannot be obtained via simple HTTP requests.\n",
        "\n",
        "### üì° 3. RapidAPI\n",
        "Query a free API endpoint (e.g., news API) to fetch JSON metadata for articles related to a topic.\n",
        "\n",
        "### üíæ 4. Data Storage\n",
        "Save all collected data as CSV files for easy future access and analysis.\n",
        "\n",
        "### üßπ 5. Cleaning & Processing\n",
        "Load CSVs via pandas, drop duplicates, handle missing values, normalize fields, and prepare data for analysis.\n",
        "\n",
        "### üìä 6. Basic Insights\n",
        "Use simple Python logic (e.g., frequency counts, word counts) to generate preliminary analysis.\n",
        "\n",
        "---\n",
        "\n",
        "## üß† What You‚Äôll See\n",
        "\n",
        "- Structured CSV outputs (`hacker_news.csv`, `twitter_ai_tweets.csv`, `rapidapi_tech_news.csv`)\n",
        "- A combined understanding of **static scraping, dynamic scraping, and API usage**\n",
        "- Cleaned and processed data\n",
        "- Basic analytical insights\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "OpHQFIOi5f6h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Request and Beautifulsoup"
      ],
      "metadata": {
        "id": "-x64T6ob5gYh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install requests beautifulsoup4 selenium pandas webdriver-manager\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "L_ijgcL59WgQ",
        "outputId": "32fa5f07-0fcf-4d76-a88b-d81f36748d0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (4.13.5)\n",
            "Collecting selenium\n",
            "  Downloading selenium-4.40.0-py3-none-any.whl.metadata (7.7 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Collecting webdriver-manager\n",
            "  Downloading webdriver_manager-4.0.2-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2026.1.4)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (2.8.1)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (4.15.0)\n",
            "Collecting trio<1.0,>=0.31.0 (from selenium)\n",
            "  Downloading trio-0.32.0-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting trio-websocket<1.0,>=0.12.2 (from selenium)\n",
            "  Downloading trio_websocket-0.12.2-py3-none-any.whl.metadata (5.1 kB)\n",
            "Collecting trio-typing>=0.10.0 (from selenium)\n",
            "  Downloading trio_typing-0.10.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting types-certifi>=2021.10.8.3 (from selenium)\n",
            "  Downloading types_certifi-2021.10.8.3-py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting types-urllib3>=1.26.25.14 (from selenium)\n",
            "  Downloading types_urllib3-1.26.25.14-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting urllib3<3,>=1.21.1 (from requests)\n",
            "  Downloading urllib3-2.6.3-py3-none-any.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: websocket-client<2.0,>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from selenium) (1.9.0)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.3)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (from webdriver-manager) (1.2.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from webdriver-manager) (25.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from trio<1.0,>=0.31.0->selenium) (25.4.0)\n",
            "Collecting sortedcontainers (from trio<1.0,>=0.31.0->selenium)\n",
            "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata (10 kB)\n",
            "Collecting outcome (from trio<1.0,>=0.31.0->selenium)\n",
            "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from trio<1.0,>=0.31.0->selenium) (1.3.1)\n",
            "Collecting mypy-extensions>=0.4.2 (from trio-typing>=0.10.0->selenium)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting async-generator (from trio-typing>=0.10.0->selenium)\n",
            "  Downloading async_generator-1.10-py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.12/dist-packages (from trio-typing>=0.10.0->selenium) (8.7.1)\n",
            "Collecting wsproto>=0.14 (from trio-websocket<1.0,>=0.12.2->selenium)\n",
            "  Downloading wsproto-1.3.2-py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.12/dist-packages (from urllib3[socks]<3.0,>=2.6.3->selenium) (1.7.1)\n",
            "Requirement already satisfied: h11<1,>=0.16.0 in /usr/local/lib/python3.12/dist-packages (from wsproto>=0.14->trio-websocket<1.0,>=0.12.2->selenium) (0.16.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata->trio-typing>=0.10.0->selenium) (3.23.0)\n",
            "Downloading selenium-4.40.0-py3-none-any.whl (9.6 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m51.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading webdriver_manager-4.0.2-py2.py3-none-any.whl (27 kB)\n",
            "Downloading trio-0.32.0-py3-none-any.whl (512 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m512.0/512.0 kB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio_typing-0.10.0-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio_websocket-0.12.2-py3-none-any.whl (21 kB)\n",
            "Downloading types_certifi-2021.10.8.3-py3-none-any.whl (2.1 kB)\n",
            "Downloading types_urllib3-1.26.25.14-py3-none-any.whl (15 kB)\n",
            "Downloading urllib3-2.6.3-py3-none-any.whl (131 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m131.6/131.6 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
            "Downloading wsproto-1.3.2-py3-none-any.whl (24 kB)\n",
            "Downloading async_generator-1.10-py3-none-any.whl (18 kB)\n",
            "Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
            "Installing collected packages: types-urllib3, types-certifi, sortedcontainers, wsproto, urllib3, outcome, mypy-extensions, async-generator, trio, webdriver-manager, trio-websocket, trio-typing, selenium\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.5.0\n",
            "    Uninstalling urllib3-2.5.0:\n",
            "      Successfully uninstalled urllib3-2.5.0\n",
            "Successfully installed async-generator-1.10 mypy-extensions-1.1.0 outcome-1.3.0.post0 selenium-4.40.0 sortedcontainers-2.4.0 trio-0.32.0 trio-typing-0.10.0 trio-websocket-0.12.2 types-certifi-2021.10.8.3 types-urllib3-1.26.25.14 urllib3-2.6.3 webdriver-manager-4.0.2 wsproto-1.3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "\n",
        "import pandas as pd\n",
        "import time\n",
        "import json\n",
        "import csv"
      ],
      "metadata": {
        "id": "u2qMd6Q69WHu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **SECTION 1: Scraping using Requests + BeautifulSoup**"
      ],
      "metadata": {
        "id": "j3TaJi73C2NU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**So we are scrapping the data from Hacker news and these are the Possible feilds we'll be working on:**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "*   Title - What article is about\n",
        "*   Article URL - External Link\n",
        "*   HN URL - Discussion Link\n",
        "*   Score (points)\n",
        "*   Author - who posted it?\n",
        "*   Post's Age\n",
        "*   Comments count\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8gEe5nrblv6P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://news.ycombinator.com/news\"\n",
        "response = requests.get(url)"
      ],
      "metadata": {
        "id": "eX7-2n2G9WkI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "soup = BeautifulSoup(response.text, \"html.parser\")"
      ],
      "metadata": {
        "id": "Y0yrP46k9Wmy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stories = soup.find_all(\"tr\", class_=\"athing\")\n",
        "print(len(stories))"
      ],
      "metadata": {
        "id": "2VfY7R8u9Wo1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1033dd0e-e100-4118-824f-9bfdaa5714cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "30\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_stories = []\n",
        "\n"
      ],
      "metadata": {
        "id": "NhYChv-F9WvB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for story in stories:\n",
        "  title_tag = story.find(\"span\", class_=\"titleline\").find(\"a\")\n",
        "  title = title_tag.get_text()\n",
        "  article_url = title_tag.get(\"href\")\n",
        "  subtext_row =  story.find_next_sibling(\"tr\")\n",
        "  score_tag = subtext_row.find(\"span\", class_=\"score\")\n",
        "  score = score_tag.get_text() if score_tag else None\n",
        "  author_tag = subtext_row.find(\"a\", class_=\"hnuser\")\n",
        "  author = author_tag.get_text() if author_tag else None\n",
        "  age_tag = subtext_row.find(\"span\", class_=\"age\")\n",
        "  age = age_tag.get_text() if age_tag else None\n",
        "  comment_tag = subtext_row.find_all(\"a\")[-1]\n",
        "  comments =comment_tag.get_text()\n",
        "  story_data = {\n",
        "    \"title\": title,\n",
        "    \"article_url\": article_url,\n",
        "    \"score\": score,\n",
        "    \"author\": author,\n",
        "    \"age\": age,\n",
        "    \"comments\": comments\n",
        "    }\n",
        "  all_stories.append(story_data)\n",
        "\n"
      ],
      "metadata": {
        "id": "rYQ28w1n9Wq2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(title)\n",
        "print(article_url)\n",
        "print(score, author, age, comments)\n",
        "print(\"-\" * 50)"
      ],
      "metadata": {
        "id": "mj8htxZw9WtH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bcbbf08-aad5-45e4-bf1a-6b97e2e20cde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Software engineers can no longer neglect their soft skills\n",
            "https://www.qu8n.com/posts/most-important-software-engineering-skill-2026\n",
            "159 points quanwinn 12 hours ago 200¬†comments\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(all_stories[0][\"title\"])\n",
        "print(all_stories[1][\"title\"])\n",
        "print(all_stories[2][\"title\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5oqwKX314pfQ",
        "outputId": "d20c0f42-9884-4003-e139-ba620a724767"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gaussian Splatting ‚Äì A$AP Rocky \"Helicopter\" music video\n",
            "Flux 2 Klein pure C inference\n",
            "Fil-Qt: A Qt Base build with Fil-C experience\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(all_stories))\n",
        "print(all_stories[1:3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ODGVrkRD16rW",
        "outputId": "90d1e045-eacd-4be9-f7a6-aee4572bd82c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "30\n",
            "[{'title': 'Flux 2 Klein pure C inference', 'article_url': 'https://github.com/antirez/flux2.c', 'score': '211 points', 'author': 'antirez', 'age': '7 hours ago', 'comments': '89\\xa0comments'}, {'title': 'Fil-Qt: A Qt Base build with Fil-C experience', 'article_url': 'https://git.qt.io/cradam/fil-qt', 'score': '15 points', 'author': 'pjmlp', 'age': '1 hour ago', 'comments': '3\\xa0comments'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(all_stories)\n",
        "df.to_csv(\"hacker_news.csv\", index=False)\n",
        "\n",
        "df.head(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        },
        "id": "tSuQZKke16uG",
        "outputId": "2bb3cb2e-70a3-4d1b-bc1a-e8324c10ccca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                               title  \\\n",
              "0  Gaussian Splatting ‚Äì A$AP Rocky \"Helicopter\" m...   \n",
              "1                      Flux 2 Klein pure C inference   \n",
              "2      Fil-Qt: A Qt Base build with Fil-C experience   \n",
              "3                                A Social Filesystem   \n",
              "4                                          Wine 11.0   \n",
              "\n",
              "                                         article_url       score  \\\n",
              "0  https://radiancefields.com/a-ap-rocky-releases...  428 points   \n",
              "1                 https://github.com/antirez/flux2.c  211 points   \n",
              "2                    https://git.qt.io/cradam/fil-qt   15 points   \n",
              "3        https://overreacted.io/a-social-filesystem/  266 points   \n",
              "4  https://gitlab.winehq.org/wine/wine/-/releases...  194 points   \n",
              "\n",
              "           author          age      comments  \n",
              "0  ChrisArchitect  7 hours ago  149¬†comments  \n",
              "1         antirez  7 hours ago   89¬†comments  \n",
              "2           pjmlp   1 hour ago    3¬†comments  \n",
              "3             icy  9 hours ago  132¬†comments  \n",
              "4             zdw  3 hours ago   35¬†comments  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-11e6299f-90f8-4565-b272-ed144134a3ad\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>article_url</th>\n",
              "      <th>score</th>\n",
              "      <th>author</th>\n",
              "      <th>age</th>\n",
              "      <th>comments</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Gaussian Splatting ‚Äì A$AP Rocky \"Helicopter\" m...</td>\n",
              "      <td>https://radiancefields.com/a-ap-rocky-releases...</td>\n",
              "      <td>428 points</td>\n",
              "      <td>ChrisArchitect</td>\n",
              "      <td>7 hours ago</td>\n",
              "      <td>149¬†comments</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Flux 2 Klein pure C inference</td>\n",
              "      <td>https://github.com/antirez/flux2.c</td>\n",
              "      <td>211 points</td>\n",
              "      <td>antirez</td>\n",
              "      <td>7 hours ago</td>\n",
              "      <td>89¬†comments</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Fil-Qt: A Qt Base build with Fil-C experience</td>\n",
              "      <td>https://git.qt.io/cradam/fil-qt</td>\n",
              "      <td>15 points</td>\n",
              "      <td>pjmlp</td>\n",
              "      <td>1 hour ago</td>\n",
              "      <td>3¬†comments</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>A Social Filesystem</td>\n",
              "      <td>https://overreacted.io/a-social-filesystem/</td>\n",
              "      <td>266 points</td>\n",
              "      <td>icy</td>\n",
              "      <td>9 hours ago</td>\n",
              "      <td>132¬†comments</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Wine 11.0</td>\n",
              "      <td>https://gitlab.winehq.org/wine/wine/-/releases...</td>\n",
              "      <td>194 points</td>\n",
              "      <td>zdw</td>\n",
              "      <td>3 hours ago</td>\n",
              "      <td>35¬†comments</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-11e6299f-90f8-4565-b272-ed144134a3ad')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-11e6299f-90f8-4565-b272-ed144134a3ad button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-11e6299f-90f8-4565-b272-ed144134a3ad');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 30,\n  \"fields\": [\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 30,\n        \"samples\": [\n          \"Starting from scratch: Training a 30M Topological Transformer\",\n          \"Dead Internet Theory\",\n          \"More sustainable epoxy thanks to phosphorus\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"article_url\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 30,\n        \"samples\": [\n          \"https://www.tuned.org.uk/posts/013_the_topological_transformer_training_tauformer\",\n          \"https://kudmitry.com/articles/dead-internet-theory/\",\n          \"https://www.empa.ch/web/s604/flamm-hemmendes-epoxidharz-nachhaltiger-machen\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"score\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 26,\n        \"samples\": [\n          \"715 points\",\n          \"33 points\",\n          \"428 points\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"author\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 28,\n        \"samples\": [\n          \"frabonacci\",\n          \"xeniafont\",\n          \"OuterVale\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"age\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 16,\n        \"samples\": [\n          \"7 hours ago\",\n          \"1 hour ago\",\n          \"20 hours ago\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"comments\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 28,\n        \"samples\": [\n          \"19\\u00a0comments\",\n          \"32\\u00a0comments\",\n          \"233\\u00a0comments\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The Reason I couldn't able to complete this Project as running SELENIUM isn't easy\n",
        "And doing the project in VS code was running but I needed Proxy to scrape the data so I didn't able to complete it**"
      ],
      "metadata": {
        "id": "f0CBRxXyRNkR"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NYam5mQVUBZx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Dynamic Job Listings Scraper using Selenium (Indeed)**\n",
        "\n",
        "\n",
        "---\n",
        "**In this Project we are scraping dynamic job listing data from Indeed using Selenium, since the content is JS-rendered, the goal is to collect structured, real world dynamic data**\n",
        "\n",
        "---\n",
        "*   Job Title ‚Äì Role being offered\n",
        "*   Company Name ‚Äì Hiring organization\n",
        "*   Job Location ‚Äì City / remote / hybrid info\n",
        "*   Job URL ‚Äì Direct link to the job posting\n",
        "*   Posted Time ‚Äì When the job was listed\n",
        "*   Scrape Timestamp ‚Äì When the data was collected"
      ],
      "metadata": {
        "id": "nzCbYtTWSt6p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "from selenium.webdriver.common.by import By\n",
        "import time\n",
        "\n"
      ],
      "metadata": {
        "id": "WPlo15I716yG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Chrome first if it's not already installed\n",
        "!wget -q -O - https://dl-ssl.google.com/linux/linux_signing_key.pub | apt-key add -\n",
        "!echo \"deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main\" | tee /etc/apt/sources.list.d/google-chrome.list\n",
        "!apt-get update\n",
        "!apt-get install -y google-chrome-stable\n",
        "\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from webdriver_manager.chrome import ChromeDriverManager\n",
        "from selenium.webdriver.common.by import By\n",
        "import time\n",
        "\n",
        "# Configure Chrome options for headless execution in Colab\n",
        "chrome_options = Options()\n",
        "chrome_options.add_argument(\"--headless\")\n",
        "chrome_options.add_argument(\"--no-sandbox\")\n",
        "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
        "\n",
        "# Setup ChromeDriver using webdriver_manager\n",
        "service = Service(ChromeDriverManager().install())\n",
        "\n",
        "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
        "query = \"dslr+camera\"\n",
        "driver.get(f\"https://www.amazon.in/s?k={query}crid=1P5SPEL9BCYHC&sprefix=dslrcamera%2Caps%2C464&ref=nb_sb_noss_2\")\n",
        "\n",
        "# Added a small delay to ensure content loads (if needed, adjust further)\n",
        "time.sleep(5)\n",
        "\n",
        "try:\n",
        "  # Using find_elements for robustness as class might appear multiple times or not at all\n",
        "  elems = driver.find_elements(By.CSS_SELECTOR, \".puis-card-container\")\n",
        "  if elems:\n",
        "    for elem in elems:\n",
        "      print(elem.text)\n",
        "  else:\n",
        "    print(\"No elements with class 'puis-card-container' found.\")\n",
        "except Exception as e:\n",
        "  print(f\"An error occurred: {e}\")\n",
        "finally:\n",
        "  driver.close()"
      ],
      "metadata": {
        "id": "Mei5AE6KPaPW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e6f5aca-7548-42cc-eb1c-0ac48825d7b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: apt-key is deprecated. Manage keyring files in trusted.gpg.d instead (see apt-key(8)).\n",
            "OK\n",
            "deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main\n",
            "Hit:1 http://dl.google.com/linux/chrome/deb stable InRelease\n",
            "Hit:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:3 https://cli.github.com/packages stable InRelease\n",
            "Hit:4 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "google-chrome-stable is already the newest version (144.0.7559.59-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 51 not upgraded.\n",
            "No elements with class 'puis-card-container' found.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aIhMOV9FPaSV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Qm-pEbPjPaU_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Vm_NwsTyPaX7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2QPxSQsePaa4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8zr2BX8ePad1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uXXXwKIKPagR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cZhqgz8aPaid"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BB97KJXzPalC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kDVkP73vPanu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GKkOrRoiPaqR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NwinrTOhPasg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y97l416uPauQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "L2Wy5o6TPawI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0dtfhZTIPax9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x4SDU8kN1665"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a2EvL7NI169E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "79778y_z16_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0wxQFSNo17BZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}